{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, classification_report, plot_confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Initialization and Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/judge-1377884607_tweet_product_company.csv',\n",
    "                 encoding=\"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                               1\n",
       "emotion_in_tweet_is_directed_at                       5802\n",
       "is_there_an_emotion_directed_at_a_brand_or_product       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                                5802\n",
       "iPad                                946\n",
       "Apple                               661\n",
       "iPad or iPhone App                  470\n",
       "Google                              430\n",
       "iPhone                              297\n",
       "Other Google product or service     293\n",
       "Android App                          81\n",
       "Android                              78\n",
       "Other Apple product or service       35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion_in_tweet_is_directed_at'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text):\n",
    "    \"\"\"This function returns a processed list of words from the given text\n",
    "    \n",
    "    This function removes html elements and urls using regular expression, then\n",
    "    converts string to list of workds, them find the stem of words in the list of words and\n",
    "    finally removes stopwords and punctuation marks from list of words.\n",
    "    \n",
    "    Args:\n",
    "        text(string): The text from which html elements, urls, stopwords, punctuation are removed and stemmed\n",
    "        \n",
    "    Returns:\n",
    "        clean_text(string): A text formed after text preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove any urls from the text\n",
    "    text = re.sub(r\"https:\\/\\/.*[\\r\\n]*\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any urls starting from www. in the text\n",
    "    text = re.sub(r\"www\\.\\w*\\.\\w\\w\\w\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any html elements from the text\n",
    "    text = re.sub(r\"<[\\w]*[\\s]*/>\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove prediods  marks\n",
    "    text = re.sub(r\"[\\.]*\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    " \n",
    "    # Initialize RegexpTokenizer\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "    \n",
    "    # Tokenize text\n",
    "    text_tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    lemmatizer  = WordNetLemmatizer()\n",
    "    # Get english stopwords\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    new_list = [\"mention\", \"sxsw\", 'link', 'rt', 'quot']\n",
    "    english_stopwords.extend(new_list)\n",
    "    \n",
    "    cleaned_text_tokens = [] # A list to hold cleaned text tokens\n",
    "    \n",
    "    for word in text_tokens:\n",
    "        if((word not in english_stopwords) and # Remove stopwords\n",
    "            (word not in string.punctuation)): # Remove punctuation marks\n",
    "                \n",
    "                lemmas = lemmatizer.lemmatize(word) # Get lemma of the current word\n",
    "                cleaned_text_tokens.append(lemmas) # Appened lemma word to list of cleaned list\n",
    "    \n",
    "    # Combine list into single string\n",
    "    clean_text = \" \".join(cleaned_text_tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df['tweet_text'] = df['tweet_text'].apply(process_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize Tweets by 'Manufacturer' based on if Tweet contains certain words\n",
    "is_apple = ['ipad', 'iphone', 'apple', 'mac', 'ios']\n",
    "is_google = ['google', 'pixel', 'android']\n",
    "\n",
    "def apple_sorter(x):\n",
    "    for i in is_apple:\n",
    "        if i.lower() in x.lower():\n",
    "            return 'Apple'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "def google_sorter(x):\n",
    "    for i in is_google:\n",
    "        if i.lower() in x.lower():\n",
    "            return 'Google'\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Manufacturer'] = df['tweet_text'].apply(apple_sorter)\n",
    "df['Google'] = df['tweet_text'].apply(google_sorter)\n",
    "df['Manufacturer'] = df['Manufacturer'].combine_first(df['Google'])\n",
    "df.drop('Google', axis=1, inplace=True)\n",
    "df.drop('emotion_in_tweet_is_directed_at', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>Manufacturer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wesley g iphone hr tweeting rise austin dead n...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jessedee know fludapp awesome ipad iphone app ...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swonderlin wait ipad also sale</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope year's festival crashy year's iphone app</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sxtxstate great stuff fri marissa mayer google...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  wesley g iphone hr tweeting rise austin dead n...   \n",
       "1  jessedee know fludapp awesome ipad iphone app ...   \n",
       "2                     swonderlin wait ipad also sale   \n",
       "3      hope year's festival crashy year's iphone app   \n",
       "4  sxtxstate great stuff fri marissa mayer google...   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product Manufacturer  \n",
       "0                                   Negative emotion        Apple  \n",
       "1                                   Positive emotion        Apple  \n",
       "2                                   Positive emotion        Apple  \n",
       "3                                   Negative emotion        Apple  \n",
       "4                                   Positive emotion       Google  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple     5548\n",
       "Google    2760\n",
       "NaN        785\n",
       "Name: Manufacturer, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Manufacturer'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['is_there_an_emotion_directed_at_a_brand_or_product'] != \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text):\n",
    "    \"\"\"This function returns a processed list of words from the given text\n",
    "    \n",
    "    This function removes html elements and urls using regular expression, then\n",
    "    converts string to list of workds, them find the stem of words in the list of words and\n",
    "    finally removes stopwords and punctuation marks from list of words.\n",
    "    \n",
    "    Args:\n",
    "        text(string): The text from which html elements, urls, stopwords, punctuation are removed and stemmed\n",
    "        \n",
    "    Returns:\n",
    "        clean_text(string): A text formed after text preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove any urls from the text\n",
    "    text = re.sub(r\"https:\\/\\/.*[\\r\\n]*\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any urls starting from www. in the text\n",
    "    text = re.sub(r\"www\\.\\w*\\.\\w\\w\\w\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any html elements from the text\n",
    "    text = re.sub(r\"<[\\w]*[\\s]*/>\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove prediods  marks\n",
    "    text = re.sub(r\"[\\.]*\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    " \n",
    "    # Initialize RegexpTokenizer\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "    \n",
    "    # Tokenize text\n",
    "    text_tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    lemmatizer  = WordNetLemmatizer()\n",
    "    # Get english stopwords\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    new_list = [\"mention\", \"sxsw\"]\n",
    "    english_stopwords.extend(new_list)\n",
    "    \n",
    "    cleaned_text_tokens = [] # A list to hold cleaned text tokens\n",
    "    \n",
    "    for word in text_tokens:\n",
    "        if((word not in english_stopwords) and # Remove stopwords\n",
    "            (word not in string.punctuation)): # Remove punctuation marks\n",
    "                \n",
    "                lemmas = lemmatizer.lemmatize(word) # Get lemma of the current word\n",
    "                cleaned_text_tokens.append(lemmas) # Appened lemma word to list of cleaned list\n",
    "    \n",
    "    # Combine list into single string\n",
    "    clean_text = \" \".join(cleaned_text_tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train['tweet_text'].apply(process_string)\n",
    "X_test = X_test['tweet_text'].apply(process_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['is_there_an_emotion_directed_at_a_brand_or_product'] = label_encoder.fit_transform(df['is_there_an_emotion_directed_at_a_brand_or_product'])\n",
    "# df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "X_train = tf_idf.fit_transform(X_train.tolist())\n",
    "X_test = tf_idf.transform(X_test.tolist())\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy/Baseliner\n",
    "estimator = DummyClassifier(strategy='most_frequent')\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions with dummy model\n",
    "y_pred = estimator.predict(X_test)\n",
    "\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(9, 4)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=estimator,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_Dummy');\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "param_grid['estimator__solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "param_grid['estimator__penalty'] = ['l2']\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(9, 4)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=estimator,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_LogReg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_contains(df):\n",
    "    i = 0\n",
    "    if df[\"tweet_text\"][i].contains(['ipad', 'iphone', 'apple', 'mac', 'ios']) in df:\n",
    "        df['emotion_in_tweet_is_directed_at'][i] = 'Apple'\n",
    "        i += 1\n",
    "    elif df[\"tweet_text\"][i].contains(['google', 'andriod', 'pixel']):\n",
    "        df['emotion_in_tweet_is_directed_at'][i] = 'Google'\n",
    "        i += 1\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_contains(df):\n",
    "    i = 0\n",
    "    for df[\"tweet_text\"][i] in df:\n",
    "        if 'ipad' or 'iphone' or 'apple' or 'mac' or 'ios' in df[\"tweet_text\"][i]:\n",
    "            df['emotion_in_tweet_is_directed_at'][i] = 'Apple'\n",
    "            i += 1\n",
    "        elif 'google' or 'andriod' or 'pixel' in df[\"tweet_text\"][i]:\n",
    "            df['emotion_in_tweet_is_directed_at'][i] = 'Google'\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.drop(df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"emotion_in_tweet_is_directed_at\", \"is_there_an_emotion_directed_at_a_brand_or_product\"])\n",
    "y = df.drop(columns=[\"emotion_in_tweet_is_directed_at\", \"tweet_text\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "print(y_train), print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "X_train = tf_idf.fit_transform(X_train[\"tweet_text\"].tolist())\n",
    "X_test = tf_idf.transform(X_test[\"tweet_text\"].tolist())\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Dummy/Baseliner\n",
    "estimator = DummyClassifier(strategy='most_frequent')\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions with dummy model\n",
    "y_pred = estimator.predict(X_test)\n",
    "\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(9, 4)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['Negative emotion', 'No emotion toward brand or product', 'Positive emotion']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=estimator,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_Dummy');\n",
    "\n",
    "target_names = ['Negative emotion', 'No emotion toward brand or product', 'Positive emotion']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "param_grid['estimator__solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "param_grid['estimator__penalty'] = ['l2']\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(9, 4)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=estimator,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_LogReg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an intance of the RegexpTokenizer with the variable name `tokenizer`\n",
    "# # The regex pattern should select all words with three or more characters\n",
    "# # pattern = r\"(?u)\\w{3,}\"\n",
    "# pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "# tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "# # Create a list of stopwords in English\n",
    "# stopwords_list = stopwords.words('english')\n",
    "\n",
    "# # Create an instance of nltk's PorterStemmer with the variable name `stemmer`\n",
    "# lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     '''\n",
    "#     Translate nltk POS to wordnet tags\n",
    "#     '''\n",
    "#     if treebank_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(df, tokenizer, stopwords_list, lemmatizer):\n",
    "#     # Standardize case (lowercase the text)\n",
    "#     lowered = df[col].lower()\n",
    "    \n",
    "#     # Tokenize text using `tokenizer`\n",
    "#     tokens = tokenizer.tokenize(lowered)\n",
    "    \n",
    "#     # Remove stopwords using `stopwords_list`\n",
    "#     stopped_tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "#     pos = pos_tag(stopped_tokens)\n",
    "#     pos = [(word[0], get_wordnet_pos(word[1])) for word in pos]\n",
    "    \n",
    "#     # Stem the tokenized text using `stemmer`\n",
    "#     lemmas = [lemmatizer.lemmatize(word[0], word[1]) for word in pos]\n",
    "    \n",
    "#     # Return the preprocessed text\n",
    "#     return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text):\n",
    "    \"\"\"This function returns a processed list of words from the given text\n",
    "    \n",
    "    This function removes html elements and urls using regular expression, then\n",
    "    converts string to list of workds, them find the stem of words in the list of words and\n",
    "    finally removes stopwords and punctuation marks from list of words.\n",
    "    \n",
    "    Args:\n",
    "        text(string): The text from which hrml elements, urls, stopwords, punctuation are removed and stemmed\n",
    "        \n",
    "    Returns:\n",
    "        clean_text(string): A text formed after text preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove any urls from the text\n",
    "    text = re.sub(r\"https:\\/\\/.*[\\r\\n]*\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any urls starting from www. in the text\n",
    "    text = re.sub(r\"www\\.\\w*\\.\\w\\w\\w\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any html elements from the text\n",
    "    text = re.sub(r\"<[\\w]*[\\s]*/>\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove prediods  marks\n",
    "    text = re.sub(r\"[\\.]*\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    " \n",
    "    # Initialize RegexpTokenizer\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "    \n",
    "    # Tokenize text\n",
    "    text_tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    lemmatizer  = WordNetLemmatizer()\n",
    "    # Get english stopwords\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    \n",
    "    cleaned_text_tokens = [] # A list to hold cleaned text tokens\n",
    "    \n",
    "    for word in text_tokens:\n",
    "        if((word not in english_stopwords) and # Remove stopwords\n",
    "            (word not in string.punctuation)): # Remove punctuation marks\n",
    "                \n",
    "                lemmas = lemmatizer.lemmatize(word) # Get lemma of the current word\n",
    "                cleaned_text_tokens.append(lemmas) # Appened lemma word to list of cleaned list\n",
    "    \n",
    "    # Combine list into single string\n",
    "    clean_text = \" \".join(cleaned_text_tokens)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "df['tweet_text'] = df['tweet_text'].apply(process_string)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['tweet_text'] = df['tweet_text'].apply(process_string)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_text'][9092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new data frame with \"id\" and \"comment\" fields\n",
    "df_subset = df[['index', 'is_there_an_emotion_directed_at_a_brand_or_product']].copy()\n",
    "#data clean-up\n",
    "#remove all non-aphabet characters\n",
    "df_subset['is_there_an_emotion_directed_at_a_brand_or_product'] = df_subset['is_there_an_emotion_directed_at_a_brand_or_product'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "#covert to lower-case\n",
    "df_subset['is_there_an_emotion_directed_at_a_brand_or_product'] = df_subset['is_there_an_emotion_directed_at_a_brand_or_product'].str.casefold()\n",
    "print (df_subset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up empty dataframe for staging output\n",
    "df1 = pd.DataFrame()\n",
    "df1['index'] = ['99999999999']\n",
    "df1['sentiment_type']='NA999NA'\n",
    "df1['sentiment_score']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "print('Processing sentiment analysis...')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "t_df = df1\n",
    "for index, row in df_subset.iterrows():\n",
    "    scores = sid.polarity_scores(row[1])\n",
    "    for key, value in scores.items():\n",
    "        temp = [key, value, row[0]]\n",
    "        df1['index'] = row[0]\n",
    "        df1['sentiment_type'] = key\n",
    "        df1['sentiment_score'] = value\n",
    "        t_df = t_df.append(df1)\n",
    "#remove dummy row with row_id = 99999999999\n",
    "t_df_cleaned = t_df[t_df.index != '99999999999']\n",
    "#remove duplicates if any exist\n",
    "t_df_cleaned = t_df_cleaned.drop_duplicates()\n",
    "# only keep rows where sentiment_type = compound\n",
    "t_df_cleaned = t_df[t_df.sentiment_type == 'compound']\n",
    "print(t_df_cleaned.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge dataframes\n",
    "df = pd.merge(df, t_df_cleaned, on='index', how='inner')\n",
    "print(df_output.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_string(df['tweet_text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "print(y_train), print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from wordcloud import WordCloud #library that allows word cloud to be generated\n",
    "\n",
    "def generate_word_cloud(dataframe_df,column_name,img_file=None,background_color=\"black\",max_words=2000):\n",
    "    \"\"\" This function generates word cloud\n",
    "    \n",
    "    Args:\n",
    "        dataframe: the datafram object which contains column whose word cloud is to be generated\n",
    "        column_name: the name of the column in dataframw whose cloud is to be generated\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #convert the column specified by column_name in dataset_df(dataframe) to list\n",
    "    sentences_list = dataframe_df[column_name].tolist()\n",
    "    \n",
    "    #convert a list to sentence\n",
    "    #sentences_single_string = \" \".join(sentences_list)\n",
    "    sentences_single_string = \" \".join(map(str,sentences_list))\n",
    "    \n",
    "    #plot word cloud\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    img_mask = None\n",
    "    \n",
    "    if img_file != None:\n",
    "        img_mask = np.array(Image.open(img_file))\n",
    "\n",
    "    wc = WordCloud(mask=img_mask,\n",
    "                   max_words=max_words,\n",
    "                   background_color=background_color)        \n",
    "        \n",
    "    plt.imshow(wc.generate(sentences_single_string),interpolation=\"bilinear\")\n",
    "    plt.title(\"Word Cloud For \" +  column_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tic= time.time()\n",
    "generate_word_cloud(X_train,\n",
    "                    \"tweet_text\",\n",
    "                    background_color=\"black\")\n",
    "toc = time.time()\n",
    "diff = 1000*(toc - tic)\n",
    "print(\"Total Time Taken: \" + str(diff) + \" ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./data/X_train.csv\")\n",
    "X_test.to_csv(\"./data/X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "\n",
    "X_train = tf_idf.fit_transform(X_train[\"tweet_text\"].tolist())\n",
    "X_test = tf_idf.transform(X_test[\"tweet_text\"].tolist())\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for record in df:\n",
    "    if 'ipad' in record['tweet_text']:\n",
    "        record['Manufacturer'] == 'Apple')\n",
    "    elif 'google' in record['tweet_text']:\n",
    "        record['Manufacturer'].replace('NaN', 'Google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = ['ipad', 'iphone', 'apple', 'mac', 'ios']\n",
    "google = ['google', 'andriod', 'pixel', 'windows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
