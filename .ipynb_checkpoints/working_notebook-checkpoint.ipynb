{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed7e83f5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d764992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, classification_report, plot_confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdece96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d408a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ca4ae",
   "metadata": {},
   "source": [
    "# Dataframe Initialization and Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bc9b95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data and initialize dataframe\n",
    "df = pd.read_csv('./data/judge-1377884607_tweet_product_company.csv',\n",
    "                 encoding=\"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660cb652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# check for nulls\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935696f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                               1\n",
       "emotion_in_tweet_is_directed_at                       5802\n",
       "is_there_an_emotion_directed_at_a_brand_or_product       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead2f1dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                                5802\n",
       "iPad                                946\n",
       "Apple                               661\n",
       "iPad or iPhone App                  470\n",
       "Google                              430\n",
       "iPhone                              297\n",
       "Other Google product or service     293\n",
       "Android App                          81\n",
       "Android                              78\n",
       "Other Apple product or service       35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view nulls vs subcategories\n",
    "df['emotion_in_tweet_is_directed_at'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06954218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>unprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[.@wesley83, I, have, a, 3G, iPhone., After, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[@jessedee, Know, about, @fludapp, ?, Awesome,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[@swonderlin, Can, not, wait, for, #iPad, 2, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[@sxsw, I, hope, this, year's, festival, isn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[@sxtxstate, great, stuff, on, Fri, #SXSW:, Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[Ipad, everywhere., #SXSW, {link}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[Wave,, buzz..., RT, @mention, We, interrupt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[Google's, Zeiger,, a, physician, never, repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[Some, Verizon, iPhone, customers, complained,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                      Negative emotion   \n",
       "1                                      Positive emotion   \n",
       "2                                      Positive emotion   \n",
       "3                                      Negative emotion   \n",
       "4                                      Positive emotion   \n",
       "...                                                 ...   \n",
       "9088                                   Positive emotion   \n",
       "9089                 No emotion toward brand or product   \n",
       "9090                 No emotion toward brand or product   \n",
       "9091                 No emotion toward brand or product   \n",
       "9092                 No emotion toward brand or product   \n",
       "\n",
       "                                       unprocessed_text  \n",
       "0     [.@wesley83, I, have, a, 3G, iPhone., After, 3...  \n",
       "1     [@jessedee, Know, about, @fludapp, ?, Awesome,...  \n",
       "2     [@swonderlin, Can, not, wait, for, #iPad, 2, a...  \n",
       "3     [@sxsw, I, hope, this, year's, festival, isn't...  \n",
       "4     [@sxtxstate, great, stuff, on, Fri, #SXSW:, Ma...  \n",
       "...                                                 ...  \n",
       "9088                 [Ipad, everywhere., #SXSW, {link}]  \n",
       "9089  [Wave,, buzz..., RT, @mention, We, interrupt, ...  \n",
       "9090  [Google's, Zeiger,, a, physician, never, repor...  \n",
       "9091  [Some, Verizon, iPhone, customers, complained,...  \n",
       "9092  [Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT,...  \n",
       "\n",
       "[9093 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_text'] = df['tweet_text'].str.split()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347bd0c",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have a large number of nulls in the data set. We will need to figure out how to handle these nulls in data cleaning in addition to the standard NLP data cleaning procedures such as RegEx, stopword removal, lemmatization,  and tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92109a9e",
   "metadata": {},
   "source": [
    "# Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20eb986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text):\n",
    "    \"\"\"This function returns a processed list of words from the given text\n",
    "    \n",
    "    This function removes html elements and urls using regular expression, then\n",
    "    converts string to list of workds, them find the stem of words in the list of words and\n",
    "    finally removes stopwords and punctuation marks from list of words.\n",
    "    \n",
    "    Args:\n",
    "        text(string): The text from which html elements, urls, stopwords, punctuation are removed and lemmatized\n",
    "        \n",
    "    Returns:\n",
    "        clean_text(string): A text formed after text preprocessing.\n",
    "    \"\"\"\n",
    "    # Remove twitter user handle from the text\n",
    "    text = re.sub('@[^\\s]+',\n",
    "                  '',\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any urls from the text\n",
    "    text = re.sub(r'https:\\/\\/.*[\\r\\n]*',\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any urls starting from www. in the text\n",
    "    text = re.sub(r'www\\.\\w*\\.\\w\\w\\w',\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any html elements from the text\n",
    "    text = re.sub(r\"<[\\w]*[\\s]*/>\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove prediods  marks\n",
    "    text = re.sub(r\"[\\.]*\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Initialize RegexpTokenizer\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "    # Tokenize text\n",
    "    text_tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Instantiate lemmatizer\n",
    "    lemmatizer  = WordNetLemmatizer()\n",
    "    \n",
    "    # Get english stopwords\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    new_list = [\"mention\", \"sxsw\", 'link', 'rt', 'quot', 'g']\n",
    "    english_stopwords.extend(new_list)\n",
    "    \n",
    "    cleaned_text_tokens = [] # A list to hold cleaned text tokens\n",
    "    \n",
    "    for word in text_tokens:\n",
    "        if((word not in english_stopwords) and # Remove stopwords\n",
    "            (word not in string.punctuation)): # Remove punctuation marks\n",
    "                \n",
    "                lemmas = lemmatizer.lemmatize(word) # Get lemma of the current word\n",
    "                cleaned_text_tokens.append(lemmas) # Appened lemma word to list of cleaned list\n",
    "    \n",
    "    # Combine list into single string\n",
    "    clean_text = \" \".join(cleaned_text_tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae5497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleanup function to tweet_text column\n",
    "df['tweet_text'] = df['tweet_text'].apply(process_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db025a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['tweet_text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b58769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize Tweets by 'Manufacturer' based on if Tweet contains certain keywords words\n",
    "# instantiate list of keywords\n",
    "is_apple = ['ipad', \n",
    "            \"ipad's\", \n",
    "            'iphone', \n",
    "            'iphones', \n",
    "            \"iphone's\", \n",
    "            'apple', \n",
    "            \"apple's\", \n",
    "            'mac', \n",
    "            'macos' \n",
    "            'ios', \n",
    "            'os' \n",
    "            'macbook', \n",
    "            'macbook pro', \n",
    "            'm1', \n",
    "            'macbook air',\n",
    "            'air',\n",
    "            'airpod', \n",
    "            'airpods',\n",
    "            'airtag'\n",
    "            'watch'\n",
    "            'monterey',\n",
    "            'big sur',\n",
    "            'catalina',\n",
    "            'mojave',\n",
    "            'high sierra',\n",
    "            'sierra',\n",
    "            'el capitan',\n",
    "            'yosemite',\n",
    "            'icloud']\n",
    "\n",
    "is_google = ['windows', \n",
    "             'google', \n",
    "             \"google's\", \n",
    "             'googles' \n",
    "             'pixel', \n",
    "             \"pixel's\", \n",
    "             'pixels', \n",
    "             'android', \n",
    "             \"android's\", \n",
    "             \"androids\", \n",
    "             'nest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2ac77d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN       5837\n",
       "Apple     2374\n",
       "Google     882\n",
       "Name: Manufacturer, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining dictionary map and mapping\n",
    "driver_operator_map = {\n",
    "    'iPad': 'Apple',\n",
    "    'Apple': 'Apple',\n",
    "    'iPad or iPhone App': 'Apple',\n",
    "    'iPhone': 'Apple',\n",
    "    'Other Apple product or service1': 'Apple',\n",
    "    \n",
    "    'Google': 'Google',\n",
    "    'Other Google product or service': 'Google',\n",
    "    'Android App': 'Google',\n",
    "    'Android': 'Google'}\n",
    "\n",
    "df['Manufacturer'] = df['emotion_in_tweet_is_directed_at'].map(driver_operator_map)\n",
    "df['Manufacturer'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd00e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions that loop through keyword lists and assign a category in a new column in the df\n",
    "def apple_sorter(x):\n",
    "    for i in is_apple:\n",
    "        if i.lower() in x.lower():\n",
    "            return 'Apple'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "def google_sorter(x):\n",
    "    for i in is_google:\n",
    "        if i.lower() in x.lower():\n",
    "            return 'Google'\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b4b0f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply apple and google sorters to tweet_text\n",
    "df['Manufacturer'] = df['tweet_text'].apply(apple_sorter)\n",
    "df['Google'] = df['tweet_text'].apply(google_sorter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "008fd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge created columns into 1 master column, 'Manufacturer'\n",
    "df['Manufacturer'] = df['Manufacturer'].combine_first(df['Google'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d0c1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop extraneous columns\n",
    "df.drop('Google', axis=1, inplace=True)\n",
    "df.drop('emotion_in_tweet_is_directed_at', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f6ee662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple     5577\n",
       "Google    2749\n",
       "NaN        767\n",
       "Name: Manufacturer, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recheck null counts\n",
    "df['Manufacturer'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "939d5e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                              0\n",
       "is_there_an_emotion_directed_at_a_brand_or_product      0\n",
       "unprocessed_text                                        1\n",
       "preprocessed_text                                       0\n",
       "Manufacturer                                          767\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb583cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop remaining NaN - does not contain major keywords related to Apple and Google\n",
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7441bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recheck null counts\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7891637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recheck null counts\n",
    "# df['Manufacturer'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c821439c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>unprocessed_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>Manufacturer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iphone hr tweeting rise austin dead need upgra...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[.@wesley83, I, have, a, 3G, iPhone., After, 3...</td>\n",
       "      <td>[iphone, hr, tweeting, rise, austin, dead, nee...</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know awesome ipad iphone app likely appreciate...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[@jessedee, Know, about, @fludapp, ?, Awesome,...</td>\n",
       "      <td>[know, awesome, ipad, iphone, app, likely, app...</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wait ipad also sale</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[@swonderlin, Can, not, wait, for, #iPad, 2, a...</td>\n",
       "      <td>[wait, ipad, also, sale]</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope year's festival crashy year's iphone app</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[@sxsw, I, hope, this, year's, festival, isn't...</td>\n",
       "      <td>[hope, year's, festival, crashy, year's, iphon...</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stuff fri marissa mayer google tim o'rei...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[@sxtxstate, great, stuff, on, Fri, #SXSW:, Ma...</td>\n",
       "      <td>[great, stuff, fri, marissa, mayer, google, ti...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  iphone hr tweeting rise austin dead need upgra...   \n",
       "1  know awesome ipad iphone app likely appreciate...   \n",
       "2                                wait ipad also sale   \n",
       "3      hope year's festival crashy year's iphone app   \n",
       "4  great stuff fri marissa mayer google tim o'rei...   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "3                                   Negative emotion   \n",
       "4                                   Positive emotion   \n",
       "\n",
       "                                    unprocessed_text  \\\n",
       "0  [.@wesley83, I, have, a, 3G, iPhone., After, 3...   \n",
       "1  [@jessedee, Know, about, @fludapp, ?, Awesome,...   \n",
       "2  [@swonderlin, Can, not, wait, for, #iPad, 2, a...   \n",
       "3  [@sxsw, I, hope, this, year's, festival, isn't...   \n",
       "4  [@sxtxstate, great, stuff, on, Fri, #SXSW:, Ma...   \n",
       "\n",
       "                                   preprocessed_text Manufacturer  \n",
       "0  [iphone, hr, tweeting, rise, austin, dead, nee...        Apple  \n",
       "1  [know, awesome, ipad, iphone, app, likely, app...        Apple  \n",
       "2                           [wait, ipad, also, sale]        Apple  \n",
       "3  [hope, year's, festival, crashy, year's, iphon...        Apple  \n",
       "4  [great, stuff, fri, marissa, mayer, google, ti...       Google  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cleaned data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5096922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.rename(columns={'tweet_text':'Text', 'is_there_an_emotion_directed_at_a_brand_or_product' : 'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e550a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check value counts for Sentiment column\n",
    "df['Sentiment'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f27496e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'I can't tell' as this will not be relevant for modeling\n",
    "df = df[df['Sentiment'] != \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5a24c9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    0.602999\n",
       "Positive emotion                      0.333221\n",
       "Negative emotion                      0.063780\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final check\n",
    "df['Sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7938292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>unprocessed_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>Manufacturer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>yup third app yet i'm android suggestion cc</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[@mention, Yup,, but, I, don't, have, a, third...</td>\n",
       "      <td>[yup, third, app, yet, i'm, android, suggestio...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>ipad everywhere</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[Ipad, everywhere., #SXSW, {link}]</td>\n",
       "      <td>[ipad, everywhere]</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>wave buzz interrupt regularly scheduled geek p...</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[Wave,, buzz..., RT, @mention, We, interrupt, ...</td>\n",
       "      <td>[wave, buzz, interrupt, regularly, scheduled, ...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>google's zeiger physician never reported poten...</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[Google's, Zeiger,, a, physician, never, repor...</td>\n",
       "      <td>[google's, zeiger, physician, never, reported,...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>verizon iphone customer complained time fell b...</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[Some, Verizon, iPhone, customers, complained,...</td>\n",
       "      <td>[verizon, iphone, customer, complained, time, ...</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "9087        yup third app yet i'm android suggestion cc   \n",
       "9088                                    ipad everywhere   \n",
       "9089  wave buzz interrupt regularly scheduled geek p...   \n",
       "9090  google's zeiger physician never reported poten...   \n",
       "9091  verizon iphone customer complained time fell b...   \n",
       "\n",
       "                               Sentiment  \\\n",
       "9087  No emotion toward brand or product   \n",
       "9088                    Positive emotion   \n",
       "9089  No emotion toward brand or product   \n",
       "9090  No emotion toward brand or product   \n",
       "9091  No emotion toward brand or product   \n",
       "\n",
       "                                       unprocessed_text  \\\n",
       "9087  [@mention, Yup,, but, I, don't, have, a, third...   \n",
       "9088                 [Ipad, everywhere., #SXSW, {link}]   \n",
       "9089  [Wave,, buzz..., RT, @mention, We, interrupt, ...   \n",
       "9090  [Google's, Zeiger,, a, physician, never, repor...   \n",
       "9091  [Some, Verizon, iPhone, customers, complained,...   \n",
       "\n",
       "                                      preprocessed_text Manufacturer  \n",
       "9087  [yup, third, app, yet, i'm, android, suggestio...       Google  \n",
       "9088                                 [ipad, everywhere]        Apple  \n",
       "9089  [wave, buzz, interrupt, regularly, scheduled, ...       Google  \n",
       "9090  [google's, zeiger, physician, never, reported,...       Google  \n",
       "9091  [verizon, iphone, customer, complained, time, ...        Apple  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(index=df.index[-1],\n",
    "        axis=0,\n",
    "        inplace=True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec547b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(df['label'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da7747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "\n",
    "# create a new data frame with \"id\" and \"comment\" fields\n",
    "df_subset = df[['index', 'Sentiment']].copy()\n",
    "\n",
    "# data clean-up\n",
    "# remove all non-aphabet characters\n",
    "df_subset['Sentiment'] = df_subset['Sentiment'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# covert to lower-case\n",
    "df_subset['Sentiment'] = df_subset['Sentiment'].str.casefold()\n",
    "# print (df_subset.head(10))\n",
    "\n",
    "# set up empty dataframe for staging output\n",
    "df1 = pd.DataFrame()\n",
    "df1['index'] = ['99999999999']\n",
    "df1['sentiment_type']='NA999NA'\n",
    "df1['sentiment_score']=0\n",
    "\n",
    "print('Processing sentiment analysis...')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "t_df = df1\n",
    "\n",
    "for index, row in df_subset.iterrows():\n",
    "    scores = sid.polarity_scores(row[1])\n",
    "\n",
    "    for key, value in scores.items():\n",
    "        temp = [key, value, row[0]]\n",
    "        df1['index'] = row[0]\n",
    "        df1['sentiment_type'] = key\n",
    "        df1['sentiment_score'] = value\n",
    "        t_df = t_df.append(df1)\n",
    "\n",
    "# remove dummy row with row_id = 99999999999\n",
    "t_df_cleaned = t_df[t_df.index != '99999999999']\n",
    "\n",
    "# remove duplicates if any exist\n",
    "t_df_cleaned = t_df_cleaned.drop_duplicates()\n",
    "\n",
    "# only keep rows where sentiment_type = compound\n",
    "t_df_cleaned = t_df[t_df.sentiment_type == 'compound']\n",
    "\n",
    "# print(t_df_cleaned.head(10))\n",
    "\n",
    "# merge dataframes\n",
    "df = pd.merge(df, t_df_cleaned, on='index', how='inner')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e752640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='sentiment_type', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb78770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length_tweet'] = df['Text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and axes\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(14, 14))\n",
    "\n",
    "# Empty dict to hold words that have already been plotted and their colors\n",
    "plotted_words_and_colors = {}\n",
    "\n",
    "# Establish color palette to pull from\n",
    "# (If you get an error message about popping from an empty list, increase this #)\n",
    "color_palette = sns.color_palette('crest', n_colors=44)\n",
    "\n",
    "# Creating a plot for each unique genre\n",
    "data_by_manufacturer = [y for _, y in df.groupby('Manufacturer', as_index=False)]\n",
    "for index, manufacturer_df in enumerate(data_by_manufacturer):\n",
    "    \n",
    "    # Find top 10 words in this genre\n",
    "    all_words_in_manufacturer = manufacturer_df.preprocessed_text.explode()\n",
    "    top_10 = all_words_in_manufacturer.value_counts()[:10]\n",
    "    \n",
    "    # Select appropriate colors, reusing colors if words repeat\n",
    "    colors = []\n",
    "    for word in top_10.index:\n",
    "        if word not in plotted_words_and_colors:\n",
    "            new_color = color_palette.pop(0)\n",
    "            plotted_words_and_colors[word] = new_color\n",
    "        colors.append(plotted_words_and_colors[word])\n",
    "    \n",
    "    # Select axes, plot data, set title\n",
    "    ax = axes[index]\n",
    "    ax.bar(top_10.index, top_10.values, color=colors)\n",
    "    ax.set_title(manufacturer_df.iloc[0].Manufacturer.title())\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('images/apple_google_top_words.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf859705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_mask = np.array(Image.open(\"./images/twitter_mask.png\"))\n",
    "twitter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06323681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a word cloud image with a twitter mask\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "new_list = [\"mention\", \"sxsw\", 'link', 'rt', 'quot' 'g']\n",
    "english_stopwords.extend(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a corpus for positive tweets only \n",
    "apple = df[df.Manufacturer == \"Apple\"]\n",
    "apple_corpus = apple.Text.to_list()\n",
    "apple_corpus = \",\".join(apple_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef4cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenize and lowercase all words \n",
    "apple_tokens = word_tokenize(apple_corpus)\n",
    "apple_stopped = [token.lower() for token in apple_tokens if token.lower() not in english_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b92d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a wordcloud with a twitter mask for positive words only \n",
    "wordcloud = WordCloud(stopwords=english_stopwords,\n",
    "                      collocations=False, \n",
    "                      mask=twitter_mask, \n",
    "                      background_color='white', \n",
    "                      width=1800,\n",
    "                      height=1400, \n",
    "                      contour_color='green', \n",
    "                      contour_width=2)\n",
    "\n",
    "wordcloud.generate(','.join(apple_stopped))\n",
    "\n",
    "plt.figure(figsize=(14, 14), \n",
    "           facecolor=None)\n",
    "\n",
    "plt.imshow(wordcloud, \n",
    "           interpolation='bilinear')\n",
    "\n",
    "plt.title('Apple Tweet Cloud', \n",
    "          size=20)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.savefig('images/apple_tweet_cloud');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a corpus for positive tweets only \n",
    "google = df[df.Manufacturer == \"Google\"]\n",
    "google_corpus = google.Text.to_list()\n",
    "google_corpus = \",\".join(google_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenize and lowercase all words \n",
    "google_tokens = word_tokenize(google_corpus)\n",
    "google_stopped = [token.lower() for token in google_tokens if token.lower() not in english_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628fd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a wordcloud with a twitter mask for positive words only \n",
    "wordcloud = WordCloud(stopwords=english_stopwords,\n",
    "                      collocations=False, \n",
    "                      mask=twitter_mask, \n",
    "                      background_color='white', \n",
    "                      width=1800,\n",
    "                      height=1400, \n",
    "                      contour_color='green', \n",
    "                      contour_width=2)\n",
    "\n",
    "wordcloud.generate(','.join(google_stopped))\n",
    "\n",
    "plt.figure(figsize=(14, 14), \n",
    "           facecolor=None)\n",
    "\n",
    "plt.imshow(wordcloud, \n",
    "           interpolation='bilinear')\n",
    "\n",
    "plt.title('Google Tweet Cloud', \n",
    "          size=20)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.savefig('images/google_tweet_cloud');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a2aa6",
   "metadata": {},
   "source": [
    "# Train/Test Split, Label Encoding, Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4c71511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split with test size = 10% and random_state for result interpretability\n",
    "X = df.Text\n",
    "y = df.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.10, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e721d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF vectorize X_train and X_test\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "X_train = tf_idf.fit_transform(X_train.tolist())\n",
    "X_test = tf_idf.transform(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00073d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8042, 41396) (894, 41396)\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48b5369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncode\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "174f80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d946f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53435952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13336330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a958fc94",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ca9b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a61aecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.Sentiment\n",
    "y_train = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "976502ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(df['Text']))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(df['Text'])\n",
    "X_train = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dff627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 25)          15400     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 25)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                1300      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,576,853\n",
      "Trainable params: 2,576,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.8883 - acc: 0.5841WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 10s 32ms/step - loss: 0.8883 - acc: 0.5841 - val_loss: 0.8113 - val_acc: 0.6085\n",
      "Epoch 2/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.8242 - acc: 0.6137WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 30ms/step - loss: 0.8242 - acc: 0.6137 - val_loss: 0.7605 - val_acc: 0.6734\n",
      "Epoch 3/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.7302 - acc: 0.6891WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.7301 - acc: 0.6891 - val_loss: 0.7607 - val_acc: 0.6689\n",
      "Epoch 4/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.7383WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6644 - acc: 0.7384 - val_loss: 0.7308 - val_acc: 0.6756\n",
      "Epoch 5/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.6164 - acc: 0.7569WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 30ms/step - loss: 0.6164 - acc: 0.7569 - val_loss: 0.7707 - val_acc: 0.6823\n",
      "Epoch 6/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.7693WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 9s 35ms/step - loss: 0.5791 - acc: 0.7695 - val_loss: 0.7658 - val_acc: 0.6655\n",
      "Epoch 7/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.5428 - acc: 0.7840WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 29ms/step - loss: 0.5425 - acc: 0.7841 - val_loss: 0.7619 - val_acc: 0.6767\n",
      "Epoch 8/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.8013WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 29ms/step - loss: 0.5052 - acc: 0.8014 - val_loss: 0.7929 - val_acc: 0.6600\n",
      "Epoch 9/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4624 - acc: 0.8243WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 31ms/step - loss: 0.4624 - acc: 0.8243 - val_loss: 0.8079 - val_acc: 0.6700\n",
      "Epoch 10/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8372WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 29ms/step - loss: 0.4432 - acc: 0.8372 - val_loss: 0.7974 - val_acc: 0.6532\n",
      "Epoch 11/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.8411WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.4180 - acc: 0.8412 - val_loss: 0.8269 - val_acc: 0.6499\n",
      "Epoch 12/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8526WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 31ms/step - loss: 0.4029 - acc: 0.8528 - val_loss: 0.8773 - val_acc: 0.6409\n",
      "Epoch 13/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8601WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 30ms/step - loss: 0.3767 - acc: 0.8601 - val_loss: 0.8662 - val_acc: 0.6611\n",
      "Epoch 14/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8616WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 30ms/step - loss: 0.3685 - acc: 0.8616 - val_loss: 0.9262 - val_acc: 0.6454\n",
      "Epoch 15/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8687WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.3538 - acc: 0.8688 - val_loss: 1.0504 - val_acc: 0.5928\n",
      "Epoch 16/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.3487 - acc: 0.8743WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 30ms/step - loss: 0.3487 - acc: 0.8743 - val_loss: 1.0101 - val_acc: 0.6163\n",
      "Epoch 17/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8781WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 30ms/step - loss: 0.3351 - acc: 0.8780 - val_loss: 1.0723 - val_acc: 0.6007\n",
      "Epoch 18/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.3244 - acc: 0.8831WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 8s 30ms/step - loss: 0.3244 - acc: 0.8831 - val_loss: 0.9946 - val_acc: 0.6040\n",
      "Epoch 19/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.8806WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 29ms/step - loss: 0.3192 - acc: 0.8806 - val_loss: 1.0197 - val_acc: 0.6320\n",
      "Epoch 20/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.8876WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 6s 25ms/step - loss: 0.3169 - acc: 0.8876 - val_loss: 1.1108 - val_acc: 0.5861\n",
      "Epoch 21/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.3056 - acc: 0.8896WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 26ms/step - loss: 0.3056 - acc: 0.8896 - val_loss: 1.0753 - val_acc: 0.6051\n",
      "Epoch 22/25\n",
      "250/252 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.8931WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 7s 28ms/step - loss: 0.2999 - acc: 0.8932 - val_loss: 1.1204 - val_acc: 0.6107\n",
      "Epoch 23/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.2947 - acc: 0.8947WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 7s 26ms/step - loss: 0.2947 - acc: 0.8947 - val_loss: 1.1365 - val_acc: 0.6051\n",
      "Epoch 24/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.2857 - acc: 0.8974WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 12s 46ms/step - loss: 0.2857 - acc: 0.8974 - val_loss: 1.2326 - val_acc: 0.5884\n",
      "Epoch 25/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.8952WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "252/252 [==============================] - 11s 45ms/step - loss: 0.2815 - acc: 0.8952 - val_loss: 1.1661 - val_acc: 0.6074\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 128\n",
    "model.add(Embedding(20000, \n",
    "                    embedding_size))\n",
    "model.add(LSTM(25, \n",
    "               return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, \n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, \n",
    "                activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "checkpoint = ModelCheckpoint(\"best_model_lstm.hdf5\", \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='auto', \n",
    "                             period=1,\n",
    "                             save_weights_only=False)\n",
    "\n",
    "print(model.summary())\n",
    "history = model.fit(X_train,\n",
    "                     y_train, \n",
    "                     epochs=25, \n",
    "                     validation_split=.1, \n",
    "                     callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 128\n",
    "model.add(Embedding(20000, embedding_size))\n",
    "model.add(layers.SimpleRNN(15, return_sequences=True))\n",
    "model.add(layers.SimpleRNN(15))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "checkpoint = ModelCheckpoint(\"best_model_rnn.hdf5\", \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='auto', \n",
    "                             period=1,\n",
    "                             save_weights_only=False)\n",
    "print(model.summary())\n",
    "history = model.fit(X_train,\n",
    "                     y_train, \n",
    "                     epochs=25, \n",
    "                     validation_split=.1, \n",
    "                     callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a03d840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 128\n",
    "model.add(Embedding(20000, embedding_size))\n",
    "model.add(layers.Conv1D(20, \n",
    "                        6, \n",
    "                        activation='relu',\n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),\n",
    "                        bias_regularizer=regularizers.l2(2e-3)))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(20, \n",
    "                        6, \n",
    "                        activation='relu',\n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),\n",
    "                        bias_regularizer=regularizers.l2(2e-3)))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(3,\n",
    "                       activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "checkpoint = ModelCheckpoint(\"best_model_cnn.hdf5\", \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='auto', \n",
    "                             period=1,\n",
    "                             save_weights_only=False)\n",
    "print(model.summary())\n",
    "history = model.fit(X_train,\n",
    "                     y_train, \n",
    "                     epochs=25, \n",
    "                     validation_split=.1, \n",
    "                     callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77701ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.8272 - accuracy: 0.6095\n",
      "Epoch 1: val_accuracy improved from -inf to 0.65213, saving model to best_model2.hdf5\n",
      "252/252 [==============================] - 11s 38ms/step - loss: 0.8272 - accuracy: 0.6095 - val_loss: 0.7475 - val_accuracy: 0.6521\n",
      "Epoch 2/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.7303 - accuracy: 0.6796\n",
      "Epoch 2: val_accuracy improved from 0.65213 to 0.65548, saving model to best_model2.hdf5\n",
      "252/252 [==============================] - 9s 36ms/step - loss: 0.7303 - accuracy: 0.6796 - val_loss: 0.7537 - val_accuracy: 0.6555\n",
      "Epoch 3/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.6655 - accuracy: 0.7123\n",
      "Epoch 3: val_accuracy improved from 0.65548 to 0.68009, saving model to best_model2.hdf5\n",
      "252/252 [==============================] - 9s 37ms/step - loss: 0.6655 - accuracy: 0.7123 - val_loss: 0.7086 - val_accuracy: 0.6801\n",
      "Epoch 4/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.6166 - accuracy: 0.7413\n",
      "Epoch 4: val_accuracy did not improve from 0.68009\n",
      "252/252 [==============================] - 10s 39ms/step - loss: 0.6167 - accuracy: 0.7412 - val_loss: 0.7101 - val_accuracy: 0.6734\n",
      "Epoch 5/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.5725 - accuracy: 0.7623\n",
      "Epoch 5: val_accuracy improved from 0.68009 to 0.68456, saving model to best_model2.hdf5\n",
      "252/252 [==============================] - 11s 42ms/step - loss: 0.5724 - accuracy: 0.7622 - val_loss: 0.7203 - val_accuracy: 0.6846\n",
      "Epoch 6/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.5365 - accuracy: 0.7814\n",
      "Epoch 6: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 44ms/step - loss: 0.5365 - accuracy: 0.7814 - val_loss: 0.7308 - val_accuracy: 0.6667\n",
      "Epoch 7/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.5068 - accuracy: 0.7928\n",
      "Epoch 7: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 12s 46ms/step - loss: 0.5068 - accuracy: 0.7928 - val_loss: 0.7381 - val_accuracy: 0.6689\n",
      "Epoch 8/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.8069\n",
      "Epoch 8: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 45ms/step - loss: 0.4839 - accuracy: 0.8069 - val_loss: 0.7717 - val_accuracy: 0.6756\n",
      "Epoch 9/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4677 - accuracy: 0.8149\n",
      "Epoch 9: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 45ms/step - loss: 0.4675 - accuracy: 0.8148 - val_loss: 0.7728 - val_accuracy: 0.6767\n",
      "Epoch 10/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.8238\n",
      "Epoch 10: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 12s 49ms/step - loss: 0.4523 - accuracy: 0.8238 - val_loss: 0.7870 - val_accuracy: 0.6689\n",
      "Epoch 11/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4312 - accuracy: 0.8308\n",
      "Epoch 11: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 44ms/step - loss: 0.4312 - accuracy: 0.8309 - val_loss: 0.7646 - val_accuracy: 0.6779\n",
      "Epoch 12/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4174 - accuracy: 0.8345\n",
      "Epoch 12: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 44ms/step - loss: 0.4174 - accuracy: 0.8345 - val_loss: 0.7857 - val_accuracy: 0.6577\n",
      "Epoch 13/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4069 - accuracy: 0.8403\n",
      "Epoch 13: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 42ms/step - loss: 0.4066 - accuracy: 0.8405 - val_loss: 0.8382 - val_accuracy: 0.6611\n",
      "Epoch 14/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3950 - accuracy: 0.8500\n",
      "Epoch 14: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 12s 46ms/step - loss: 0.3948 - accuracy: 0.8500 - val_loss: 0.8105 - val_accuracy: 0.6521\n",
      "Epoch 15/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.8540\n",
      "Epoch 15: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 45ms/step - loss: 0.3858 - accuracy: 0.8540 - val_loss: 0.8185 - val_accuracy: 0.6600\n",
      "Epoch 16/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.8540\n",
      "Epoch 16: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 12s 47ms/step - loss: 0.3811 - accuracy: 0.8540 - val_loss: 0.8137 - val_accuracy: 0.6745\n",
      "Epoch 17/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.3661 - accuracy: 0.8601\n",
      "Epoch 17: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 12s 47ms/step - loss: 0.3661 - accuracy: 0.8601 - val_loss: 0.8638 - val_accuracy: 0.6477\n",
      "Epoch 18/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3552 - accuracy: 0.8648\n",
      "Epoch 18: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 12s 46ms/step - loss: 0.3553 - accuracy: 0.8646 - val_loss: 0.8483 - val_accuracy: 0.6644\n",
      "Epoch 19/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8693\n",
      "Epoch 19: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 12s 48ms/step - loss: 0.3506 - accuracy: 0.8693 - val_loss: 0.8508 - val_accuracy: 0.6655\n",
      "Epoch 20/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3465 - accuracy: 0.8685\n",
      "Epoch 20: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 45ms/step - loss: 0.3463 - accuracy: 0.8687 - val_loss: 0.8476 - val_accuracy: 0.6644\n",
      "Epoch 21/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3398 - accuracy: 0.8729\n",
      "Epoch 21: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 10s 42ms/step - loss: 0.3405 - accuracy: 0.8725 - val_loss: 0.8792 - val_accuracy: 0.6521\n",
      "Epoch 22/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.8778\n",
      "Epoch 22: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 44ms/step - loss: 0.3324 - accuracy: 0.8778 - val_loss: 0.8793 - val_accuracy: 0.6555\n",
      "Epoch 23/25\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.8769\n",
      "Epoch 23: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 44ms/step - loss: 0.3283 - accuracy: 0.8769 - val_loss: 0.8677 - val_accuracy: 0.6734\n",
      "Epoch 24/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8801\n",
      "Epoch 24: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 44ms/step - loss: 0.3220 - accuracy: 0.8803 - val_loss: 0.8629 - val_accuracy: 0.6622\n",
      "Epoch 25/25\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8799\n",
      "Epoch 25: val_accuracy did not improve from 0.68456\n",
      "252/252 [==============================] - 11s 45ms/step - loss: 0.3209 - accuracy: 0.8800 - val_loss: 0.8921 - val_accuracy: 0.6678\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 128\n",
    "model.add(Embedding(20000, \n",
    "                    embedding_size))\n",
    "model.add(layers.Bidirectional(layers.LSTM(20, \n",
    "                                           dropout=0.6)))\n",
    "model.add(layers.Dense(3, \n",
    "                       activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "checkpoint = ModelCheckpoint(\"best_model_bidir.hdf5\", \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='auto', \n",
    "                             period=1,\n",
    "                             save_weights_only=False)\n",
    "print(model.summary())\n",
    "history = model.fit(X_train,\n",
    "                     y_train, \n",
    "                     epochs=25, \n",
    "                     validation_split=.1, \n",
    "                     callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e537864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789aee0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7321b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382da617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⏰ This cell may take about thirty seconds to run\n",
    "\n",
    "# Raw text complaints\n",
    "tweets = df['Text'] \n",
    "\n",
    "# Initialize a tokenizer \n",
    "tokenizer = Tokenizer(num_words=2000) \n",
    "\n",
    "# Fit it to the complaints\n",
    "tokenizer.fit_on_texts(tweets) \n",
    "\n",
    "# Generate sequences\n",
    "sequences = tokenizer.texts_to_sequences(tweets) \n",
    "print('sequences type:', type(sequences))\n",
    "\n",
    "# Similar to sequences, but returns a numpy array\n",
    "one_hot_results= tokenizer.texts_to_matrix(tweets, mode='binary') \n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "\n",
    "# Useful if we wish to decode (more explanation below)\n",
    "word_index = tokenizer.word_index \n",
    "\n",
    "# Tokens are the number of unique words across the corpus\n",
    "print('Found %s unique tokens.' % len(word_index)) \n",
    "\n",
    "# Our coded data\n",
    "print('Dimensions of our coded results:', np.shape(one_hot_results)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec04afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "reverse_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_idx_to_preview = 19\n",
    "print('Original complaint text:')\n",
    "print(tweets[comment_idx_to_preview])\n",
    "print('\\n\\n')\n",
    "\n",
    "# The reverse_index cell block above must be complete in order for this cell block to successively execute \n",
    "decoded_review = ' '.join([reverse_index.get(i) for i in sequences[comment_idx_to_preview]])\n",
    "print('Decoded review from Tokenizer:')\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd862f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df['Sentiment']\n",
    "\n",
    "# Initialize\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(sentiment)\n",
    "print('Original class labels:')\n",
    "print(list(le.classes_))\n",
    "print('\\n')\n",
    "product_cat = le.transform(sentiment)  \n",
    "\n",
    "# If you wish to retrieve the original descriptive labels post production\n",
    "# list(le.inverse_transform([0, 1, 3, 3, 0, 6, 4])) \n",
    "\n",
    "print('New sentiment labels:')\n",
    "print(product_cat)\n",
    "print('\\n')\n",
    "\n",
    "# Each row will be all zeros except for the category for that observation \n",
    "print('One hot labels; 3 binary columns, one for each of the categories.') \n",
    "product_onehot = to_categorical(product_cat)\n",
    "print(product_onehot)\n",
    "print('\\n')\n",
    "\n",
    "print('One hot labels shape:')\n",
    "print(np.shape(product_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "test_index = random.sample(range(1, 8936), 1500)\n",
    "\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "print('Test label shape:', np.shape(label_test))\n",
    "print('Train label shape:', np.shape(label_train))\n",
    "print('Test shape:', np.shape(test))\n",
    "print('Train shape:', np.shape(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sequential model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Two layers with relu activation\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# One layer with softmax activation \n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70029fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "history = model.fit(train,\n",
    "                    label_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1cfe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss vs the number of epoch\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "visual = sns.lineplot(x=epochs, \n",
    "                      y=loss_values, \n",
    "                      label='Training Loss', \n",
    "                      color='#00A36C')\n",
    "visual.set_title('Training Loss')\n",
    "visual.set_xlabel('Epochs')\n",
    "visual.set_ylabel('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training accuracy vs the number of epochs\n",
    "accuracy_values = history_dict['acc']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "visual = sns.lineplot(x=epochs, \n",
    "                      y=loss_values, \n",
    "                      label='Training Accuracy', \n",
    "                      color='#FF3131')\n",
    "visual.set_title('Training Accuracy')\n",
    "visual.set_xlabel('Epochs')\n",
    "visual.set_ylabel('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output (probability) predictions for the test set \n",
    "y_hat_test = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss and accuracy for the training set \n",
    "results_train = model.evaluate(train, label_train)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss and accuracy for the test set \n",
    "results_test = model.evaluate(test, label_test)\n",
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad2656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6c85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b1159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6265f9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db156a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8c19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96aadf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77312de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46188935",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eeb75d",
   "metadata": {},
   "source": [
    "#### Baseline Model (Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92125064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy/Baseliner\n",
    "estimator = DummyClassifier(strategy='most_frequent')\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions with dummy model\n",
    "y_pred = estimator.predict(X_test)\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['Pos (+)', 'Neutral', 'Neg (-)']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=estimator,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_Dummy');\n",
    "\n",
    "target_names = ['Pos (+)', 'Neutral', 'Neg (-)']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033fbcf",
   "metadata": {},
   "source": [
    "Dummy model predicts 'Neutral' for all and has an accuracy of 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad87fb2c",
   "metadata": {},
   "source": [
    "#### Logistic Regression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7166968",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# strategy = {0:2000, 1:4200, 2:2657}\n",
    "\n",
    "# pipe = ImPipeline(steps=[\n",
    "#     ('sm', SMOTE(random_state=42,\n",
    "#                  sampling_strategy=strategy)),\n",
    "#     ('estimator', LogisticRegression(random_state=42))\n",
    "# ])\n",
    "\n",
    "# param_grid = {}\n",
    "# param_grid['estimator__C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "# param_grid['estimator__solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "# param_grid['estimator__penalty'] = ['l2']\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=pipe, \n",
    "#                            param_grid=param_grid, \n",
    "#                            cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "#                            return_train_score=True, \n",
    "#                            scoring='accuracy', \n",
    "#                            n_jobs=-1,\n",
    "#                            verbose=2)\n",
    "\n",
    "# # Fit models run gridsearch\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Mean training score\n",
    "# grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# # Mean test score\n",
    "# grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "# best_grid = grid_search.best_estimator_\n",
    "# best_grid.fit(X_train, y_train)\n",
    "# y_pred = best_grid.predict(X_test)\n",
    "\n",
    "# print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "# print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "# print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "# print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "\n",
    "# # Set figsize and font scale\n",
    "# sns.set(rc={'figure.figsize':(8, 8)})\n",
    "# sns.set(font_scale=1)\n",
    "\n",
    "# # Set diply labesl for confusion matrix\n",
    "# display_labels = ['Pos (+)', 'Neutral', 'Neg (-)']\n",
    "\n",
    "# # Plot a confusion matrix on the test data\n",
    "# plot_confusion_matrix(estimator=best_grid,\n",
    "#                       X=X_test,\n",
    "#                       y_true=y_test,\n",
    "#                       display_labels=display_labels)\n",
    "\n",
    "\n",
    "# target_names = ['Pos (+)', 'Neutral', 'Neg (-)']\n",
    "# print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# # Save confusion matrix as png and place it in the images folder\n",
    "# plt.savefig('images/Confusion_Matrix_LogRegSMOTE');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859d4b5",
   "metadata": {},
   "source": [
    "#### Logistic without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce020eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "param_grid['estimator__solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "param_grid['estimator__penalty'] = ['l2']\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['Pos (+)', 'Neutral', 'Neg (-)']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "\n",
    "target_names = ['Pos (+)', 'Neutral', 'Neg (-)']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_LogReg');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784470ac",
   "metadata": {},
   "source": [
    "Performs better on unseen data without SMOTE, we will not use SMOTE in our next models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f4657b",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d832b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', RidgeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__alpha'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['Pos (+)', 'Neutral', 'Neg (-)']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['Pos (+)', 'Neutral', 'Neg (-)']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_Ridge');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e061da",
   "metadata": {},
   "source": [
    "Ridge yields a more overfit model, but it performs slightly better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f49b2",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d389d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ImPipeline(steps=[\n",
    "    ('estimator', KNeighborsClassifier())])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__n_neighbors'] = [1, 5, 9, 13, 17, 21]\n",
    "param_grid['estimator__metric'] = ['euclidean', 'manhattan', 'minkowski']\n",
    "param_grid['estimator__weights'] = ['uniform', 'distance']\n",
    "    \n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['Pos(+)', 'Neutral', 'Neg(-)']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['Pos(+)', 'Neutral', 'Neg(-)']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_KNN');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c0403",
   "metadata": {},
   "source": [
    "KNN not good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a45251",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7244c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ImPipeline(steps=[\n",
    "    ('estimator', SVC())])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__kernel'] = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid['estimator__C'] = [10, 1.0, 0.1]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['Pos(+)', 'Neutral', 'Neg(-)']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['Pos(+)', 'Neutral', 'Neg(-)']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_SVM');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623c4a5",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "param_grid['estimator__max_features'] = ['auto', 'sqrt', 'log2']\n",
    "param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                                 return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 n_iter=100, \n",
    "                                 random_state=42, \n",
    "                                 n_jobs=-1, \n",
    "                                 verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['Pos(+)', 'Neutral', 'Neg(-)']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['Pos(+)', 'Neutral', 'Neg(-)']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_RandomForest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', MLPClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__learning_rate'] = [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "param_grid['estimator__hidden_layer_sizes'] = [(100,1), (100,2), (100,3)]\n",
    "param_grid['estimator__alpha'] = [10.0 ** -np.arange(1, 9)]\n",
    "param_grid['estimator__activation'] = [\"logistic\", \"relu\", \"Tanh\"]\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                                 return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 n_iter=100, \n",
    "                                 random_state=42, \n",
    "                                 n_jobs=-1, \n",
    "                                 verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['Pos(+)', 'Neutral', 'Neg(-)']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['Pos(+)', 'Neutral', 'Neg(-)']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_RandomForest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97a753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c3602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb23d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f02c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ea4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
