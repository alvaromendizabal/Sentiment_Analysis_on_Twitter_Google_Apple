{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9158c9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f70325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, classification_report, plot_confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3be2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.9\n",
      "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.8.0\n",
      "    Uninstalling keras-2.8.0:\n",
      "      Successfully uninstalled keras-2.8.0\n",
      "Successfully installed keras-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you'll have keras 2.9.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install keras==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6838119",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (C:\\Users\\alvaro\\anaconda3\\envs\\learn-env\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-fc20ded4c09f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPool1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalMaxPool1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\keras\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayout_map\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\keras\\dtensor\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Conditional import the dtensor API, since it is currently broken in OSS.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_DTENSOR_API_ENABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdtensor_api\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[1;31m# Leave it with a placeholder, so that the import line from other python file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (C:\\Users\\alvaro\\anaconda3\\envs\\learn-env\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3ef4b",
   "metadata": {},
   "source": [
    "# Dataframe Initialization and Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ef993d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data and initialize dataframe\n",
    "df = pd.read_csv('./data/judge-1377884607_tweet_product_company.csv',\n",
    "                 encoding=\"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f47205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# check for nulls\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca1a0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                               1\n",
       "emotion_in_tweet_is_directed_at                       5802\n",
       "is_there_an_emotion_directed_at_a_brand_or_product       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2e34ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                                5802\n",
       "iPad                                946\n",
       "Apple                               661\n",
       "iPad or iPhone App                  470\n",
       "Google                              430\n",
       "iPhone                              297\n",
       "Other Google product or service     293\n",
       "Android App                          81\n",
       "Android                              78\n",
       "Other Apple product or service       35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view nulls vs subcategories\n",
    "df['emotion_in_tweet_is_directed_at'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66737c",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have a large number of nulls in the data set. We will need to figure out how to handle these nulls in data cleaning in addition to the standard NLP data cleaning procedures such as RegEx, stopword removal, lemmatization,  and tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e689f",
   "metadata": {},
   "source": [
    "# Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29523bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text):\n",
    "    \"\"\"This function returns a processed list of words from the given text\n",
    "    \n",
    "    This function removes html elements and urls using regular expression, then\n",
    "    converts string to list of workds, them find the stem of words in the list of words and\n",
    "    finally removes stopwords and punctuation marks from list of words.\n",
    "    \n",
    "    Args:\n",
    "        text(string): The text from which html elements, urls, stopwords, punctuation are removed and lemmatized\n",
    "        \n",
    "    Returns:\n",
    "        clean_text(string): A text formed after text preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove any urls from the text\n",
    "    text = re.sub(r'https:\\/\\/.*[\\r\\n]*',\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any urls starting from www. in the text\n",
    "    text = re.sub(r'www\\.\\w*\\.\\w\\w\\w',\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove any html elements from the text\n",
    "    text = re.sub(r\"<[\\w]*[\\s]*/>\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    "    # Remove prediods  marks\n",
    "    text = re.sub(r\"[\\.]*\",\n",
    "                  \"\",\n",
    "                  str(text))\n",
    "    \n",
    " \n",
    "    # Initialize RegexpTokenizer\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "    \n",
    "    # Tokenize text\n",
    "    text_tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    lemmatizer  = WordNetLemmatizer()\n",
    "    # Get english stopwords\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    new_list = [\"mention\", \"sxsw\", 'link', 'rt', 'quot']\n",
    "    english_stopwords.extend(new_list)\n",
    "    \n",
    "    cleaned_text_tokens = [] # A list to hold cleaned text tokens\n",
    "    \n",
    "    for word in text_tokens:\n",
    "        if((word not in english_stopwords) and # Remove stopwords\n",
    "            (word not in string.punctuation)): # Remove punctuation marks\n",
    "                \n",
    "                lemmas = lemmatizer.lemmatize(word) # Get lemma of the current word\n",
    "                cleaned_text_tokens.append(lemmas) # Appened lemma word to list of cleaned list\n",
    "    \n",
    "    # Combine list into single string\n",
    "    clean_text = \" \".join(cleaned_text_tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3452a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleanup function to tweet_text column\n",
    "df['tweet_text'] = df['tweet_text'].apply(process_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eaa9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize Tweets by 'Manufacturer' based on if Tweet contains certain keywords words\n",
    "# instantiate list of keywords\n",
    "is_apple = ['ipad', \"ipad's\", 'iphone', 'iphones', \"iphone's\", 'apple', \"apple's\", 'mac', 'ios']\n",
    "is_google = ['google', \"google's\", 'pixel', \"pixel's\", 'pixels', 'android', \"android's\", \"androids\", 'nest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c00889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions that loop through keyword lists and assign a category in a new column in the df\n",
    "def apple_sorter(x):\n",
    "    for i in is_apple:\n",
    "        if i.lower() in x.lower():\n",
    "            return 'Apple'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "def google_sorter(x):\n",
    "    for i in is_google:\n",
    "        if i.lower() in x.lower():\n",
    "            return 'Google'\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840b2067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply apple and google sorters to tweet_text\n",
    "df['Manufacturer'] = df['tweet_text'].apply(apple_sorter)\n",
    "df['Google'] = df['tweet_text'].apply(google_sorter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe980dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge created columns into 1 master column, 'Manufacturer'\n",
    "df['Manufacturer'] = df['Manufacturer'].combine_first(df['Google'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "348072c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop extraneous columns\n",
    "df.drop('Google', axis=1, inplace=True)\n",
    "df.drop('emotion_in_tweet_is_directed_at', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80019e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple     5548\n",
       "Google    2762\n",
       "NaN        783\n",
       "Name: Manufacturer, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recheck null counts\n",
    "df['Manufacturer'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5487d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop remaining NaN - does not contain major keywords related to Apple and Google\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57a0b950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple     5548\n",
       "Google    2762\n",
       "Name: Manufacturer, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recheck null counts\n",
    "df['Manufacturer'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e6b9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>Manufacturer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wesley g iphone hr tweeting rise austin dead n...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jessedee know fludapp awesome ipad iphone app ...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swonderlin wait ipad also sale</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope year's festival crashy year's iphone app</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sxtxstate great stuff fri marissa mayer google...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  wesley g iphone hr tweeting rise austin dead n...   \n",
       "1  jessedee know fludapp awesome ipad iphone app ...   \n",
       "2                     swonderlin wait ipad also sale   \n",
       "3      hope year's festival crashy year's iphone app   \n",
       "4  sxtxstate great stuff fri marissa mayer google...   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product Manufacturer  \n",
       "0                                   Negative emotion        Apple  \n",
       "1                                   Positive emotion        Apple  \n",
       "2                                   Positive emotion        Apple  \n",
       "3                                   Negative emotion        Apple  \n",
       "4                                   Positive emotion       Google  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cleaned data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c531969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.rename(columns={'tweet_text':'Text', 'is_there_an_emotion_directed_at_a_brand_or_product' : 'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "709f1778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Manufacturer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wesley g iphone hr tweeting rise austin dead n...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jessedee know fludapp awesome ipad iphone app ...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swonderlin wait ipad also sale</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope year's festival crashy year's iphone app</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sxtxstate great stuff fri marissa mayer google...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text         Sentiment  \\\n",
       "0  wesley g iphone hr tweeting rise austin dead n...  Negative emotion   \n",
       "1  jessedee know fludapp awesome ipad iphone app ...  Positive emotion   \n",
       "2                     swonderlin wait ipad also sale  Positive emotion   \n",
       "3      hope year's festival crashy year's iphone app  Negative emotion   \n",
       "4  sxtxstate great stuff fri marissa mayer google...  Positive emotion   \n",
       "\n",
       "  Manufacturer  \n",
       "0        Apple  \n",
       "1        Apple  \n",
       "2        Apple  \n",
       "3        Apple  \n",
       "4       Google  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "872ce945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    4651\n",
       "Positive emotion                      2940\n",
       "Negative emotion                       569\n",
       "I can't tell                           150\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check value counts for Sentiment column\n",
    "df['Sentiment'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db075bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'I can't tell' as this will not be relevant for modeling\n",
    "df = df[df['Sentiment'] != \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "518c330d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    0.569975\n",
       "Positive emotion                      0.360294\n",
       "Negative emotion                      0.069730\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final check\n",
    "df['Sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed1169",
   "metadata": {},
   "source": [
    "# Train/Test Split, Label Encoding, Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04fe9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split with test size = 33% and random_state for result interpretability\n",
    "X = df.Text\n",
    "y = df.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5924fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF vectorize X_train and X_test\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "X_train = tf_idf.fit_transform(X_train.tolist())\n",
    "X_test = tf_idf.transform(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c949a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shapes\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88afce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncode\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4352cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd226b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad4e029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gabacustweets', 'ipad', 'sold', 'went', 'new', 'buyer', 'report', 'socialmedia', 'brk'], ['google', 'coming', 'location', 'dominance', 'pcmagcom'], ['holler', 'gram', 'ipad', 'itunes', 'app', 'store', 'via', 'sters', 'great', 'app', 'madebymany', 'free'], ['get', 'badge', 'find', 'food', 'drink', 'figure', 'iphone', 'roaming', 'unpack', 'priority'], ['wonder', 'many', 'ipads', \"they'll\", 'sell', 'opening', 'pop', 'store'], ['help', 'win', 'ipad', 'click', 'like', 'following', 'picture', 'party', 'thanks'], ['google', 'test', 'check', 'offer', 'via', 'google', 'check'], ['deviantart', 'buy', 'ipad', 'austin', 'test', 'muro', 'drawing', 'super', 'fast', 'deviantart', 'htt', 'cont'], ['google', 'asks', 'want', 'know']]\n"
     ]
    }
   ],
   "source": [
    "wordvec = []\n",
    "\n",
    "for i in X_train:\n",
    "    wordvec.append(i.split())\n",
    "    \n",
    "print(wordvec[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "322c37af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=1496, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "word_2_vec = Word2Vec(sentences=wordvec, \n",
    "                      size=100, \n",
    "                      window=5, \n",
    "                      min_count=5, \n",
    "                      workers=16)\n",
    "print(word_2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f128cf4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2552056e519a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1496\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "token = Tokenizer(1496)\n",
    "token.fit_on_texts(df['Text'])\n",
    "text = token.texts_to_sequences(df['Text'])\n",
    "text = pad_sequences(text, 75)\n",
    "print(text[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = Sequential()\n",
    "keras_model.add(word_2_vec.wv.get_keras_embedding(True))\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(50, 3, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(50, 3, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(MaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(100, 3, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(100, 3, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(MaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(200, 3, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(200, 3, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(GlobalMaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Dense(200))\n",
    "keras_model.add(Activation('relu'))\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Dense(2))\n",
    "keras_model.add(Activation('softmax'))\n",
    "keras_model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "keras_model.fit(X_train, y_train, batch_size=16, epochs=3, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f03c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfed974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba34df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e574d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dca3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a84132e8",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a87580",
   "metadata": {},
   "source": [
    "#### Baseline Model (Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21768bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy/Baseliner\n",
    "estimator = DummyClassifier(strategy='most_frequent')\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions with dummy model\n",
    "y_pred = estimator.predict(X_test)\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=estimator,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_Dummy');\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ef26f",
   "metadata": {},
   "source": [
    "Dummy model predicts 'Neutral' for all and has an accuracy of 57%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5cd77",
   "metadata": {},
   "source": [
    "#### Logistic Regression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f3e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = ImPipeline(steps=[\n",
    "    ('sm', SMOTE(random_state=42)),\n",
    "    ('estimator', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "param_grid['estimator__solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "param_grid['estimator__penalty'] = ['l2']\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_LogReg');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307434d3",
   "metadata": {},
   "source": [
    "Performs well on Training Data, but may be overfit due to its worse performance on unseen Test Data, let's see the results for the same pipeline without SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca56cf2",
   "metadata": {},
   "source": [
    "#### Logistic without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255446d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = ImPipeline(steps=[\n",
    "    ('estimator', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "param_grid['estimator__solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "param_grid['estimator__penalty'] = ['l2']\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_LogReg_NoSMOTE');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfe3ca",
   "metadata": {},
   "source": [
    "Performs better on unseen data without SMOTE, we will not use SMOTE in our next models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe0976",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1bef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = ImPipeline(steps=[\n",
    "    ('estimator', RidgeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__alpha'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_Ridge');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9072d3",
   "metadata": {},
   "source": [
    "Ridge yields a more overfit model, but it performs slightly better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b58bbc",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ImPipeline(steps=[\n",
    "    ('estimator', KNeighborsClassifier())])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__n_neighbors'] = [1, 5, 9, 13, 17, 21]\n",
    "param_grid['estimator__metric'] = ['euclidean', 'manhattan', 'minkowski']\n",
    "param_grid['estimator__weights'] = ['uniform', 'distance']\n",
    "    \n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_KNN');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e45c5e",
   "metadata": {},
   "source": [
    "KNN not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ImPipeline(steps=[\n",
    "    ('estimator', SVC())])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__kernel'] = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid['estimator__C'] = [10, 1.0, 0.1]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_KNN');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4c033",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "param_grid['estimator__max_features'] = ['auto', 'sqrt', 'log2']\n",
    "param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                                 return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 n_iter=100, \n",
    "                                 random_state=42, \n",
    "                                 n_jobs=-1, \n",
    "                                 verbose=2)\n",
    "\n",
    "# Fit models run gridsearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "# Set figsize and font scale\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "# Set diply labesl for confusion matrix\n",
    "display_labels = ['-', 'Neutral', '+']\n",
    "\n",
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(estimator=best_grid,\n",
    "                      X=X_test,\n",
    "                      y_true=y_test,\n",
    "                      display_labels=display_labels)\n",
    "\n",
    "target_names = ['-', 'Neutral', '+']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Save confusion matrix as png and place it in the images folder\n",
    "plt.savefig('images/Confusion_Matrix_RandomForest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183b534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
